{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a22ce5c6",
   "metadata": {},
   "source": [
    "# Question Answering – Fine-tuning – DistilBERT-base\n",
    "\n",
    "\n",
    "Ce notebook entraîne un modèle de Question Answering extractif\n",
    "sur les données SQuAD préprocessées.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "988c1424",
   "metadata": {},
   "source": [
    "## Objectifs\n",
    "\n",
    "- Charger les données préprocessées\n",
    "- Charger un modèle QA pré-entraîné\n",
    "- Fine-tuner avec Trainer\n",
    "- Sauvegarder le meilleur modèle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ce1288",
   "metadata": {},
   "source": [
    "## Pourquoi DistilBERT ?\n",
    "\n",
    "DistilBERT est une version compressée de BERT, obtenue par distillation de connaissances.\n",
    "Il contient moins de paramètres (40 % de réduction), ce qui permet un temps d'entraînement\n",
    "et d'inférence plus rapide, au prix d'une légère baisse de performance attendue.\n",
    "Cela le rend idéal pour une comparaison équitable avec BERT et RoBERTa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "25af6076",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aissi\\OneDrive - De Vinci\\A5\\UVSQ\\S2\\Fouille de données\\qa-project\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForQuestionAnswering,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DefaultDataCollator\n",
    ")\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99a9cce9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import set_seed\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d1db69bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA disponible: True\n",
      "GPU: NVIDIA GeForce RTX 3060 Laptop GPU\n",
      "Nombre de GPUs: 1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"CUDA disponible: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Nombre de GPUs: {torch.cuda.device_count()}\")\n",
    "else:\n",
    "    print(\"Pas de GPU disponible - utilisation du CPU\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f40ec4",
   "metadata": {},
   "source": [
    "## Chargement des données préprocessées\n",
    "\n",
    "Les données ont été prétraitées dans le notebook précédent et sont chargées directement \n",
    "depuis le disque. On utilise une version réduite du dataset SQuAD afin de limiter le \n",
    "temps de calcul tout en conservant la structure du problème de Question Answering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5c377e7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets = load_from_disk(\"outputs/tokenized_squad_small\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "208dfc75",
   "metadata": {},
   "source": [
    "## Data collator\n",
    "\n",
    "Permet de créer les batchs correctement.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3ce2201e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DefaultDataCollator()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd5f6b88",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "\n",
    "Le tokenizer associé au modèle est chargé pour assurer la cohérence\n",
    "du pipeline d’entraînement et d’inférence.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d229cab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5ff53f",
   "metadata": {},
   "source": [
    "## Modèle\n",
    "\n",
    "On charge un modèle compatible Question Answering.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1bf08ce7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aissi\\OneDrive - De Vinci\\A5\\UVSQ\\S2\\Fouille de données\\qa-project\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\aissi\\.cache\\huggingface\\hub\\models--distilbert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Loading weights: 100%|██████████| 100/100 [00:00<00:00, 294.17it/s, Materializing param=distilbert.transformer.layer.5.sa_layer_norm.weight]   \n",
      "DistilBertForQuestionAnswering LOAD REPORT from: distilbert-base-uncased\n",
      "Key                     | Status     | \n",
      "------------------------+------------+-\n",
      "vocab_layer_norm.bias   | UNEXPECTED | \n",
      "vocab_transform.bias    | UNEXPECTED | \n",
      "vocab_transform.weight  | UNEXPECTED | \n",
      "vocab_projector.bias    | UNEXPECTED | \n",
      "vocab_layer_norm.weight | UNEXPECTED | \n",
      "qa_outputs.weight       | MISSING    | \n",
      "qa_outputs.bias         | MISSING    | \n",
      "\n",
      "Notes:\n",
      "- UNEXPECTED\t:can be ignored when loading from different task/architecture; not ok if you expect identical arch.\n",
      "- MISSING\t:those params were newly initialized because missing from the checkpoint. Consider training on your downstream task.\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b27d0328",
   "metadata": {},
   "source": [
    "## Paramètres d’entraînement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef8ac0c",
   "metadata": {},
   "source": [
    "Grâce au nombre réduit de paramètres de DistilBERT, un batch size plus élevé (16) \n",
    "peut être utilisé sans dépasser les limites mémoire du GPU, optimisant ainsi l'utilisation\n",
    "des ressources disponibles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64213000",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`logging_dir` is deprecated and will be removed in v5.2. Please set `TENSORBOARD_LOGGING_DIR` instead.\n"
     ]
    }
   ],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"outputs/checkpoints/distilbert\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=3e-5,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=8,\n",
    "    num_train_epochs=1,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"outputs/logs/distilbert\",\n",
    "    logging_steps=50,\n",
    "    save_total_limit=1,\n",
    "    load_best_model_at_end=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1c44c3",
   "metadata": {},
   "source": [
    "## Entraînement\n",
    "\n",
    "On lance le fine-tuning avec Trainer.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9be246d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"validation\"],\n",
    "    data_collator=data_collator\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bc81c950",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 03:34, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.958214</td>\n",
       "      <td>3.812345</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  1.74it/s]\n",
      "There were missing keys in the checkpoint model loaded: ['distilbert.embeddings.LayerNorm.weight', 'distilbert.embeddings.LayerNorm.bias'].\n",
      "There were unexpected keys in the checkpoint model loaded: ['distilbert.embeddings.LayerNorm.beta', 'distilbert.embeddings.LayerNorm.gamma'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=125, training_loss=4.292656433105469, metrics={'train_runtime': 217.2468, 'train_samples_per_second': 9.206, 'train_steps_per_second': 0.575, 'total_flos': 195979650048000.0, 'train_loss': 4.292656433105469, 'epoch': 1.0})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fa36a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.state.log_history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "332155a7",
   "metadata": {},
   "source": [
    "## Sauvegarde"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53f516fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Writing model shards: 100%|██████████| 1/1 [00:00<00:00,  1.32it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('outputs/checkpoints/distilbert/final\\\\tokenizer_config.json',\n",
       " 'outputs/checkpoints/distilbert/final\\\\tokenizer.json')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.save_model(\"outputs/checkpoints/distilbert/final\")\n",
    "tokenizer.save_pretrained(\"outputs/checkpoints/distilbert/final\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576cc72d",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Le modèle a été fine-tuné et sauvegardé.\n",
    "La prochaine étape consistera à évaluer la qualité (EM/F1) et la latence,\n",
    "puis à comparer 3 modèles.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
