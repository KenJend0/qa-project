{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6a4097a",
   "metadata": {},
   "source": [
    "# Question Answering – Preprocessing des données\n",
    "\n",
    "Ce notebook a pour objectif de préparer les données du dataset SQuAD\n",
    "pour l’entraînement d’un modèle Transformer en question answering\n",
    "extractif.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f611cd6",
   "metadata": {},
   "source": [
    "## Objectifs\n",
    "\n",
    "- Tokeniser les paires (question, contexte)\n",
    "- Gérer les contextes longs par découpage\n",
    "- Aligner les positions de début et de fin de la réponse\n",
    "  avec les tokens générés par le tokenizer\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0973dd74",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aissi\\OneDrive - De Vinci\\A5\\UVSQ\\S2\\Fouille de données\\qa-project\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f618c9e",
   "metadata": {},
   "source": [
    "## Chargement du dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "145465e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"squad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d14c9fa4",
   "metadata": {},
   "source": [
    "## Initialisation du tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "329b4ef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aissi\\OneDrive - De Vinci\\A5\\UVSQ\\S2\\Fouille de données\\qa-project\\.venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:130: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\aissi\\.cache\\huggingface\\hub\\models--distilbert-base-uncased. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    }
   ],
   "source": [
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ce686f8",
   "metadata": {},
   "source": [
    "## Paramètres de tokenization\n",
    "\n",
    "Les contextes pouvant être longs, nous utilisons :\n",
    "- une longueur maximale\n",
    "- un stride pour le découpage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95ace767",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 384\n",
    "doc_stride = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cba40950",
   "metadata": {},
   "source": [
    "## Fonction de preprocessing\n",
    "\n",
    "Cette fonction permet :\n",
    "- de tokeniser les données\n",
    "- de gérer les contextes longs\n",
    "- d’aligner les positions des réponses avec les tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0667b9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_function(examples):\n",
    "    questions = [q.strip() for q in examples[\"question\"]]\n",
    "\n",
    "    inputs = tokenizer(\n",
    "        questions,\n",
    "        examples[\"context\"],\n",
    "        max_length=max_length,\n",
    "        truncation=\"only_second\",\n",
    "        stride=doc_stride,\n",
    "        return_overflowing_tokens=True,\n",
    "        return_offsets_mapping=True,\n",
    "        padding=\"max_length\",\n",
    "    )\n",
    "\n",
    "    offset_mapping = inputs.pop(\"offset_mapping\")\n",
    "    sample_mapping = inputs.pop(\"overflow_to_sample_mapping\")\n",
    "\n",
    "    start_positions = []\n",
    "    end_positions = []\n",
    "\n",
    "    for i, offsets in enumerate(offset_mapping):\n",
    "        input_ids = inputs[\"input_ids\"][i]\n",
    "        cls_index = input_ids.index(tokenizer.cls_token_id)\n",
    "\n",
    "        sequence_ids = inputs.sequence_ids(i)\n",
    "        sample_index = sample_mapping[i]\n",
    "\n",
    "        answers = examples[\"answers\"][sample_index]\n",
    "        if len(answers[\"answer_start\"]) == 0:\n",
    "            start_positions.append(cls_index)\n",
    "            end_positions.append(cls_index)\n",
    "        else:\n",
    "            start_char = answers[\"answer_start\"][0]\n",
    "            end_char = start_char + len(answers[\"text\"][0])\n",
    "\n",
    "            token_start_index = 0\n",
    "            while sequence_ids[token_start_index] != 1:\n",
    "                token_start_index += 1\n",
    "\n",
    "            token_end_index = len(input_ids) - 1\n",
    "            while sequence_ids[token_end_index] != 1:\n",
    "                token_end_index -= 1\n",
    "\n",
    "            if not (offsets[token_start_index][0] <= start_char and\n",
    "                    offsets[token_end_index][1] >= end_char):\n",
    "                start_positions.append(cls_index)\n",
    "                end_positions.append(cls_index)\n",
    "            else:\n",
    "                while token_start_index < len(offsets) and offsets[token_start_index][0] <= start_char:\n",
    "                    token_start_index += 1\n",
    "                start_positions.append(token_start_index - 1)\n",
    "\n",
    "                while offsets[token_end_index][1] >= end_char:\n",
    "                    token_end_index -= 1\n",
    "                end_positions.append(token_end_index + 1)\n",
    "\n",
    "    inputs[\"start_positions\"] = start_positions\n",
    "    inputs[\"end_positions\"] = end_positions\n",
    "\n",
    "    return inputs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "308d8cab",
   "metadata": {},
   "source": [
    "**Remarque** : Pour simplifier l'entraînement, nous utilisons la première réponse annotée (`answers[\"answer_start\"][0]`) lorsqu'il y en a plusieurs. Cela est volontairement adapté à SQuAD v1, où généralement une réponse est privilégiée."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "848e54d9",
   "metadata": {},
   "source": [
    "## Application du preprocessing au dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "40f97452",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Tokenizing SQuAD: 100%|██████████| 87599/87599 [00:56<00:00, 1539.99 examples/s]\n",
      "Tokenizing SQuAD: 100%|██████████| 10570/10570 [00:06<00:00, 1595.91 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    "    remove_columns=dataset[\"train\"].column_names,\n",
    "    desc=\"Tokenizing SQuAD\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa324e64",
   "metadata": {},
   "source": [
    "Le preprocessing est appliqué de manière identique aux ensembles d'entraînement et de validation afin de garantir la comparabilité des performances."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec49338a",
   "metadata": {},
   "source": [
    "## Vérification des données préprocessées\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0eba68fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [101,\n",
       "  2000,\n",
       "  3183,\n",
       "  2106,\n",
       "  1996,\n",
       "  6261,\n",
       "  2984,\n",
       "  9382,\n",
       "  3711,\n",
       "  1999,\n",
       "  8517,\n",
       "  1999,\n",
       "  10223,\n",
       "  26371,\n",
       "  2605,\n",
       "  1029,\n",
       "  102,\n",
       "  6549,\n",
       "  2135,\n",
       "  1010,\n",
       "  1996,\n",
       "  2082,\n",
       "  2038,\n",
       "  1037,\n",
       "  3234,\n",
       "  2839,\n",
       "  1012,\n",
       "  10234,\n",
       "  1996,\n",
       "  2364,\n",
       "  2311,\n",
       "  1005,\n",
       "  1055,\n",
       "  2751,\n",
       "  8514,\n",
       "  2003,\n",
       "  1037,\n",
       "  3585,\n",
       "  6231,\n",
       "  1997,\n",
       "  1996,\n",
       "  6261,\n",
       "  2984,\n",
       "  1012,\n",
       "  3202,\n",
       "  1999,\n",
       "  2392,\n",
       "  1997,\n",
       "  1996,\n",
       "  2364,\n",
       "  2311,\n",
       "  1998,\n",
       "  5307,\n",
       "  2009,\n",
       "  1010,\n",
       "  2003,\n",
       "  1037,\n",
       "  6967,\n",
       "  6231,\n",
       "  1997,\n",
       "  4828,\n",
       "  2007,\n",
       "  2608,\n",
       "  2039,\n",
       "  14995,\n",
       "  6924,\n",
       "  2007,\n",
       "  1996,\n",
       "  5722,\n",
       "  1000,\n",
       "  2310,\n",
       "  3490,\n",
       "  2618,\n",
       "  4748,\n",
       "  2033,\n",
       "  18168,\n",
       "  5267,\n",
       "  1000,\n",
       "  1012,\n",
       "  2279,\n",
       "  2000,\n",
       "  1996,\n",
       "  2364,\n",
       "  2311,\n",
       "  2003,\n",
       "  1996,\n",
       "  13546,\n",
       "  1997,\n",
       "  1996,\n",
       "  6730,\n",
       "  2540,\n",
       "  1012,\n",
       "  3202,\n",
       "  2369,\n",
       "  1996,\n",
       "  13546,\n",
       "  2003,\n",
       "  1996,\n",
       "  24665,\n",
       "  23052,\n",
       "  1010,\n",
       "  1037,\n",
       "  14042,\n",
       "  2173,\n",
       "  1997,\n",
       "  7083,\n",
       "  1998,\n",
       "  9185,\n",
       "  1012,\n",
       "  2009,\n",
       "  2003,\n",
       "  1037,\n",
       "  15059,\n",
       "  1997,\n",
       "  1996,\n",
       "  24665,\n",
       "  23052,\n",
       "  2012,\n",
       "  10223,\n",
       "  26371,\n",
       "  1010,\n",
       "  2605,\n",
       "  2073,\n",
       "  1996,\n",
       "  6261,\n",
       "  2984,\n",
       "  22353,\n",
       "  2135,\n",
       "  2596,\n",
       "  2000,\n",
       "  3002,\n",
       "  16595,\n",
       "  9648,\n",
       "  4674,\n",
       "  2061,\n",
       "  12083,\n",
       "  9711,\n",
       "  2271,\n",
       "  1999,\n",
       "  8517,\n",
       "  1012,\n",
       "  2012,\n",
       "  1996,\n",
       "  2203,\n",
       "  1997,\n",
       "  1996,\n",
       "  2364,\n",
       "  3298,\n",
       "  1006,\n",
       "  1998,\n",
       "  1999,\n",
       "  1037,\n",
       "  3622,\n",
       "  2240,\n",
       "  2008,\n",
       "  8539,\n",
       "  2083,\n",
       "  1017,\n",
       "  11342,\n",
       "  1998,\n",
       "  1996,\n",
       "  2751,\n",
       "  8514,\n",
       "  1007,\n",
       "  1010,\n",
       "  2003,\n",
       "  1037,\n",
       "  3722,\n",
       "  1010,\n",
       "  2715,\n",
       "  2962,\n",
       "  6231,\n",
       "  1997,\n",
       "  2984,\n",
       "  1012,\n",
       "  102,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'token_type_ids': [0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'attention_mask': [1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  1,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0,\n",
       "  0],\n",
       " 'start_positions': 130,\n",
       " 'end_positions': 137}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_datasets[\"train\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d45d008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vérification de la validité des positions\n",
    "assert all(\n",
    "    0 <= s <= e < max_length\n",
    "    for s, e in zip(\n",
    "        tokenized_datasets[\"train\"][\"start_positions\"],\n",
    "        tokenized_datasets[\"train\"][\"end_positions\"]\n",
    "    )\n",
    "), \"Erreur : positions invalides détectées\"\n",
    "\n",
    "print(\"Toutes les positions (start ≤ end) sont valides\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2110d7b0",
   "metadata": {},
   "source": [
    "## Gestion des réponses hors fenêtre\n",
    "\n",
    "Lorsque la réponse ne se trouve pas dans la fenêtre tokenisée (cas du sliding window), les positions de début et de fin sont fixées sur le token `[CLS]` (index 0).\n",
    "\n",
    "Cela permet au modèle d'apprendre à prédire l'absence de réponse dans un segment donné."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "423901f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exemple d'une fenêtre sans réponse (index 6):\n",
      "  start_positions: 98\n",
      "  end_positions: 98\n",
      "  input_ids (premiers 20): [101, 2129, 2411, 2003, 10289, 8214, 1005, 1055, 1996, 26536, 17420, 2405, 1029, 102, 2004, 2012, 2087, 2060, 5534, 1010]\n"
     ]
    }
   ],
   "source": [
    "# Trouver un exemple où la réponse est absente (ou hors fenêtre)\n",
    "cls_positions = [\n",
    "    i for i, (s, e) in enumerate(\n",
    "        zip(\n",
    "            tokenized_datasets[\"train\"][\"start_positions\"],\n",
    "            tokenized_datasets[\"train\"][\"end_positions\"]\n",
    "        )\n",
    "    )\n",
    "    if s == e  # Cas où start == end (réponse hors fenêtre ou absente)\n",
    "]\n",
    "\n",
    "if cls_positions:\n",
    "    idx = cls_positions[0]\n",
    "    print(f\"Exemple d'une fenêtre sans réponse (index {idx}):\")\n",
    "    print(f\"  start_positions: {tokenized_datasets['train'][idx]['start_positions']}\")\n",
    "    print(f\"  end_positions: {tokenized_datasets['train'][idx]['end_positions']}\")\n",
    "    print(f\"  input_ids (premiers 20): {tokenized_datasets['train'][idx]['input_ids'][:20]}\")\n",
    "else:\n",
    "    print(\"Aucun exemple de réponse hors fenêtre trouvé\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f049ae92",
   "metadata": {},
   "source": [
    "## Inspection des features finales\n",
    "\n",
    "Les données finales contiennent uniquement les entrées nécessaires à l'entraînement du modèle Transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e591fbab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features du dataset d'entraînement:\n",
      "{'input_ids': List(Value('int32')), 'token_type_ids': List(Value('int8')), 'attention_mask': List(Value('int8')), 'start_positions': Value('int64'), 'end_positions': Value('int64')}\n",
      "\n",
      "Nombre d'exemples train: 88524\n",
      "Nombre d'exemples validation: 10784\n"
     ]
    }
   ],
   "source": [
    "print(\"Features du dataset d'entraînement:\")\n",
    "print(tokenized_datasets[\"train\"].features)\n",
    "print(f\"\\nNombre d'exemples train: {len(tokenized_datasets['train'])}\")\n",
    "print(f\"Nombre d'exemples validation: {len(tokenized_datasets['validation'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de73cfa6",
   "metadata": {},
   "source": [
    "## Sauvegarde des données préprocessées\n",
    "\n",
    "Les données tokenisées sont sauvegardées afin d’être réutilisées\n",
    "directement lors de l’entraînement et de l’évaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d54d3339",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (0/1 shards):   0%|          | 0/88524 [00:00<?, ? examples/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 88524/88524 [00:00<00:00, 250350.49 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 10784/10784 [00:00<00:00, 220094.37 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_datasets.save_to_disk(\"outputs/tokenized_squad\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41f9f5e8",
   "metadata": {},
   "source": [
    "## Création d'un dataset réduit pour l'entraînement\n",
    "\n",
    "Pour optimiser le temps d'entraînement et de validation, nous créons un sous-ensemble représentatif du dataset SQuAD (2000 exemples train, 500 validation)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dd634e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_datasets_small = tokenized_datasets.shuffle(seed=42)\n",
    "tokenized_datasets_small[\"train\"] = tokenized_datasets_small[\"train\"].select(range(2000))\n",
    "tokenized_datasets_small[\"validation\"] = tokenized_datasets_small[\"validation\"].select(range(500))\n",
    "\n",
    "tokenized_datasets_small.save_to_disk(\"outputs/tokenized_squad_small\")\n",
    "\n",
    "print(f\"Dataset réduit créé:\")\n",
    "print(f\"  Train: {len(tokenized_datasets_small['train'])} exemples\")\n",
    "print(f\"  Validation: {len(tokenized_datasets_small['validation'])} exemples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75ba9d9c",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Les données ont été correctement tokenisées et les positions des réponses\n",
    "ont été alignées avec les tokens.\n",
    "\n",
    "Ces données peuvent maintenant être utilisées pour l’entraînement\n",
    "d’un modèle Transformer en question answering extractif.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
