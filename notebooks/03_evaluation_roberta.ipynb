{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d0e4033",
   "metadata": {},
   "source": [
    "# Question Answering – Évaluation du modèle\n",
    "\n",
    "Ce notebook évalue les performances du modèle fine-tuné\n",
    "sur le dataset SQuAD (Exact Match, F1-score et temps d’inférence).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3048dfed",
   "metadata": {},
   "source": [
    "## Objectifs\n",
    "\n",
    "- Charger le modèle fine-tuné\n",
    "- Évaluer les performances sur le jeu de validation\n",
    "- Calculer les métriques Exact Match et F1\n",
    "- Mesurer le temps d’inférence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f6fdea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\aissi\\OneDrive - De Vinci\\A5\\UVSQ\\S2\\Fouille de données\\qa-project\\.venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83172f68",
   "metadata": {},
   "source": [
    "## Paramètres d'évaluation\n",
    "\n",
    "**Contexte :**\n",
    "- L'évaluation est effectuée sur un sous-ensemble de 500 exemples afin de limiter le temps de calcul tout en conservant une estimation représentative des performances.\n",
    "- Pour des raisons de temps de calcul, l'évaluation est réalisée sur une seule fenêtre de contexte (sans sliding window), ce qui peut légèrement sous-estimer les performances réelles.\n",
    "- Seules les métriques **Exact Match (EM)** et **F1** sont utilisées, conformément à la norme d'évaluation SQuAD."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3bf7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load original SQuAD data for evaluation (answers are not in tokenized_datasets)\n",
    "from datasets import load_dataset\n",
    "raw_datasets = load_dataset(\"squad\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad53c07",
   "metadata": {},
   "source": [
    "## Chargement du modèle fine-tuné\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c6e6c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading weights: 100%|██████████| 199/199 [00:00<00:00, 330.51it/s, Materializing param=roberta.encoder.layer.11.output.dense.weight]              \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "RobertaForQuestionAnswering(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_path = \"outputs/checkpoints/roberta/final\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_path)\n",
    "\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5523769f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaForQuestionAnswering(\n",
       "  (roberta): RobertaModel(\n",
       "    (embeddings): RobertaEmbeddings(\n",
       "      (word_embeddings): Embedding(50265, 768, padding_idx=1)\n",
       "      (token_type_embeddings): Embedding(1, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "      (position_embeddings): Embedding(514, 768, padding_idx=1)\n",
       "    )\n",
       "    (encoder): RobertaEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x RobertaLayer(\n",
       "          (attention): RobertaAttention(\n",
       "            (self): RobertaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): RobertaSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): RobertaIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): RobertaOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377357f4",
   "metadata": {},
   "source": [
    "## Métriques SQuAD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2520da38",
   "metadata": {},
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"squad\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e3e16f",
   "metadata": {},
   "source": [
    "## Fonction d’inférence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9a1a55b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_score(example):\n",
    "    inputs = {\n",
    "        \"input_ids\": torch.tensor(example[\"input_ids\"]).unsqueeze(0).to(device),\n",
    "        \"attention_mask\": torch.tensor(example[\"attention_mask\"]).unsqueeze(0).to(device)\n",
    "    }\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    start_logits = outputs.start_logits.squeeze()\n",
    "    end_logits = outputs.end_logits.squeeze()\n",
    "\n",
    "    start_idx = torch.argmax(start_logits)\n",
    "    end_idx = torch.argmax(end_logits)\n",
    "\n",
    "    score = start_logits[start_idx] + end_logits[end_idx]\n",
    "\n",
    "    return start_idx.item(), end_idx.item(), score.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb6bca3",
   "metadata": {},
   "source": [
    "## Évaluation sur le jeu de validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ac89c07f",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples = 500\n",
    "validation_set = raw_datasets[\"validation\"].select(range(min(n_samples, len(raw_datasets[\"validation\"]))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe16012f",
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "references = []\n",
    "y_true = []\n",
    "y_scores = []\n",
    "\n",
    "for example in validation_set:\n",
    "    # Tokenize the raw example\n",
    "    encoded = tokenizer(\n",
    "        example[\"question\"],\n",
    "        example[\"context\"],\n",
    "        truncation=True,\n",
    "        max_length=384,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    # Get predictions\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**{k: v.to(device) for k, v in encoded.items()})\n",
    "    \n",
    "    start_logits = outputs.start_logits[0]\n",
    "    end_logits = outputs.end_logits[0]\n",
    "    \n",
    "    start_pred = torch.argmax(start_logits).item()\n",
    "    end_pred = torch.argmax(end_logits).item()\n",
    "    \n",
    "    # Fix invalid span\n",
    "    if end_pred < start_pred:\n",
    "        end_pred = start_pred\n",
    "    \n",
    "    score = start_logits[start_pred] + end_logits[end_pred]\n",
    "\n",
    "    # Decode prediction\n",
    "    prediction_text = tokenizer.decode(\n",
    "        encoded[\"input_ids\"][0][start_pred:end_pred + 1],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    gold_text = example[\"answers\"][\"text\"][0]\n",
    "\n",
    "    # Exact Match → label binaire\n",
    "    y_true.append(int(prediction_text.strip() == gold_text.strip()))\n",
    "    y_scores.append(score.item())\n",
    "\n",
    "    # Append to predictions and references for metric computation\n",
    "    predictions.append({\n",
    "        \"id\": example[\"id\"],\n",
    "        \"prediction_text\": prediction_text\n",
    "    })\n",
    "    \n",
    "    references.append({\n",
    "        \"id\": example[\"id\"],\n",
    "        \"answers\": example[\"answers\"]\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515e3d60",
   "metadata": {},
   "source": [
    "## Résultats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eefaca7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'exact_match': 0.2, 'f1': 3.642459670237826}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = metric.compute(predictions=predictions, references=references)\n",
    "results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0675e3d4",
   "metadata": {},
   "source": [
    "## Mesure du temps d’inférence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5492158a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.03647572088241577"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for example in validation_set:\n",
    "    encoded = tokenizer(\n",
    "        example[\"question\"],\n",
    "        example[\"context\"],\n",
    "        truncation=True,\n",
    "        max_length=384,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model(**{k: v.to(device) for k, v in encoded.items()})\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "avg_time = (end_time - start_time) / n_samples\n",
    "avg_time"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02619df4",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Le modèle fine-tuné RoBERTa-base a été évalué sur le jeu de validation SQuAD.\n",
    "\n",
    "Les métriques utilisées sont :\n",
    "- **Exact Match (EM)** : pourcentage de prédictions identiques aux réponses gold\n",
    "- **F1** : moyenne harmonique de la précision et du rappel au niveau des tokens\n",
    "- **Temps d'inférence** : temps moyen pour traiter un exemple\n",
    "\n",
    "Ces deux métriques (EM et F1) constituent la norme standard d'évaluation SQuAD et sont les plus pertinentes pour cette tâche de question-répondage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35db37bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'model': 'RoBERTa-base',\n",
       " 'EM': 0.2,\n",
       " 'F1': 3.642459670237826,\n",
       " 'Precision': np.float64(0.003658063808334542),\n",
       " 'Recall': np.float64(0.5580448065173116),\n",
       " 'AUC': 0.5571142284569137,\n",
       " 'Inference_time_ms': 36.47572088241577}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_summary = {\n",
    "    \"model\": \"RoBERTa-base\",\n",
    "    \"EM\": results[\"exact_match\"],\n",
    "    \"F1\": results[\"f1\"],\n",
    "    \"Inference_time_ms\": avg_time * 1000\n",
    "}\n",
    "\n",
    "results_summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "111818fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Résultats sauvegardés dans outputs/results_roberta.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Sauvegarder les résultats en JSON\n",
    "with open(\"outputs/results_roberta.json\", \"w\") as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "\n",
    "print(\"Résultats sauvegardés dans outputs/results_roberta.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a68e8f",
   "metadata": {},
   "source": [
    "## Résumé des résultats\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
