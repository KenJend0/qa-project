{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6d0e4033",
   "metadata": {},
   "source": [
    "# Question Answering – Évaluation du modèle\n",
    "\n",
    "Ce notebook évalue les performances du modèle fine-tuné\n",
    "sur le dataset SQuAD (Exact Match, F1-score et temps d’inférence).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3048dfed",
   "metadata": {},
   "source": [
    "## Objectifs\n",
    "\n",
    "- Charger le modèle fine-tuné\n",
    "- Évaluer les performances sur le jeu de validation\n",
    "- Calculer les métriques Exact Match et F1\n",
    "- Mesurer le temps d’inférence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89f6fdea",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n",
    "import evaluate\n",
    "import numpy as np\n",
    "import time\n",
    "import torch\n",
    "from sklearn.metrics import precision_recall_curve, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83172f68",
   "metadata": {},
   "source": [
    "## Chargement des données préprocessées\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3bf7a6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "tokenized_datasets = load_from_disk(\"outputs/tokenized_squad\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad53c07",
   "metadata": {},
   "source": [
    "## Chargement du modèle fine-tuné\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c6e6c23",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "model_path = \"outputs/checkpoints/roberta/final\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModelForQuestionAnswering.from_pretrained(model_path)\n",
    "\n",
    "model.eval()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5523769f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "377357f4",
   "metadata": {},
   "source": [
    "## Métriques SQuAD\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2520da38",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "metric = evaluate.load(\"squad\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e3e16f",
   "metadata": {},
   "source": [
    "## Fonction d’inférence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1a55b9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def predict_with_score(example):\n",
    "    inputs = {\n",
    "        \"input_ids\": torch.tensor(example[\"input_ids\"]).unsqueeze(0).to(device),\n",
    "        \"attention_mask\": torch.tensor(example[\"attention_mask\"]).unsqueeze(0).to(device)\n",
    "    }\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    start_logits = outputs.start_logits.squeeze()\n",
    "    end_logits = outputs.end_logits.squeeze()\n",
    "\n",
    "    start_idx = torch.argmax(start_logits)\n",
    "    end_idx = torch.argmax(end_logits)\n",
    "\n",
    "    score = start_logits[start_idx] + end_logits[end_idx]\n",
    "\n",
    "    return start_idx.item(), end_idx.item(), score.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb6bca3",
   "metadata": {},
   "source": [
    "## Évaluation sur le jeu de validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac89c07f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "n_samples = 500\n",
    "validation_set = tokenized_datasets[\"validation\"].select(range(n_samples))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe16012f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "y_true = []\n",
    "y_scores = []\n",
    "\n",
    "for example in validation_set:\n",
    "    start_pred, end_pred, score = predict_with_score(example)\n",
    "\n",
    "    prediction_text = tokenizer.decode(\n",
    "        example[\"input_ids\"][start_pred:end_pred + 1],\n",
    "        skip_special_tokens=True\n",
    "    )\n",
    "\n",
    "    gold_text = example[\"answers\"][\"text\"][0]\n",
    "\n",
    "    # Exact Match → label binaire\n",
    "    y_true.append(int(prediction_text.strip() == gold_text.strip()))\n",
    "    y_scores.append(score)\n",
    "\n",
    "    metric.add(\n",
    "        prediction={\n",
    "            \"id\": example[\"id\"],\n",
    "            \"prediction_text\": prediction_text\n",
    "        },\n",
    "        reference={\n",
    "            \"id\": example[\"id\"],\n",
    "            \"answers\": {\n",
    "                \"text\": [gold_text],\n",
    "                \"answer_start\": [example[\"answers\"][\"answer_start\"][0]]\n",
    "            }\n",
    "        }\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515e3d60",
   "metadata": {},
   "source": [
    "## Résultats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eefaca7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "results = metric.compute()\n",
    "results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49fc28d0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=f\"AUC = {roc_auc:.3f}\")\n",
    "plt.plot([0, 1], [0, 1], linestyle=\"--\")\n",
    "plt.xlabel(\"False Positive Rate\")\n",
    "plt.ylabel(\"True Positive Rate\")\n",
    "plt.title(\"ROC Curve – Question Answering\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d1d10a",
   "metadata": {},
   "source": [
    "## Visualisation de la courbe ROC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9974d5f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "fpr, tpr, roc_thresholds = roc_curve(y_true, y_scores)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "roc_auc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf40cf1",
   "metadata": {},
   "source": [
    "## Courbe ROC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c89630e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "precision, recall, pr_thresholds = precision_recall_curve(y_true, y_scores)\n",
    "\n",
    "precision[:5], recall[:5]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a51848a9",
   "metadata": {},
   "source": [
    "## Precision et Recall\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0675e3d4",
   "metadata": {},
   "source": [
    "## Mesure du temps d’inférence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5492158a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "for example in validation_set:\n",
    "    predict(example)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "avg_time = (end_time - start_time) / n_samples\n",
    "avg_time\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02619df4",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "Le modèle fine-tuné a été évalué sur le jeu de validation SQuAD.\n",
    "\n",
    "En plus des métriques Exact Match et F1, nous avons évalué les modèles\n",
    "à l'aide des métriques Precision, Recall et AUC.\n",
    "\n",
    "La courbe ROC permet d'analyser la capacité du modèle à distinguer\n",
    "les réponses correctes des réponses incorrectes en fonction d'un\n",
    "seuil de confiance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35db37bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_summary = {\n",
    "    \"model\": \"RoBERTa-base\",\n",
    "    \"EM\": results[\"exact_match\"],\n",
    "    \"F1\": results[\"f1\"],\n",
    "    \"Precision\": precision.mean(),\n",
    "    \"Recall\": recall.mean(),\n",
    "    \"AUC\": roc_auc,\n",
    "    \"Inference_time_ms\": avg_time * 1000\n",
    "}\n",
    "\n",
    "results_summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "111818fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Sauvegarder les résultats en JSON\n",
    "with open(\"outputs/results_roberta.json\", \"w\") as f:\n",
    "    json.dump(results_summary, f, indent=2)\n",
    "\n",
    "print(\"Résultats sauvegardés dans outputs/results_roberta.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a68e8f",
   "metadata": {},
   "source": [
    "## Résumé des résultats\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
